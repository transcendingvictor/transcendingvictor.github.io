<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>About | M. Reboredo Prado</title>

    <!-- Fonts -->
    <link href="resources/fonts/Computer Modern/Sans/cmun-sans.css" rel="stylesheet">
    <link href="resources/fonts/Computer Modern/Serif/cmun-serif.css" rel="stylesheet">

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Aleo:ital,wght@0,100..900;1,100..900&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Vollkorn+SC:wght@400;600;700;900&display=swap" rel="stylesheet">

    <!-- Icons -->
    <script src="https://kit.fontawesome.com/a39fa44c6b.js" crossorigin="anonymous"></script>

    <!-- Stylesheet-->
    <link href="./resources/css/style.css" type="text/css" rel="stylesheet">
</head>

<body>
    <header>
        <div class="me">
            <a aria-current="page" href="./self.html" >
                <img class="circularimage borderimage" alt="me!" src="./resources/img/victor_abrigo_graduacion.jpg" width="100" height="100">
                <p> <strong> Víctor Abia Alonso </strong> </p>
            </a>
        </div>
        <p> 1st year PhD Student in AI Value Alignment interested in social choice, multiagent systems and moral value systems.
        <div class="links">
            <span> <a target="_blank" rel="nofollow noopener noreferrer"
                      href="https://github.com/transcendingvictor">Github</a> </span>
            <span class="tab"></span>
            <span> <a target="_blank" rel="nofollow noopener noreferrer"
                      href="https://www.linkedin.com/in/v%C3%ADctor-abia-alonso-8492a31bb/">LinkedIn</a> </span>
            <span class="tab"></span>
            <span> <a target="_blank" rel="nofollow noopener noreferrer"
                      href="https://orcid.org/0009-0004-1775-8405">ORCID</a> </span>
            <span class="tab"></span>
            <span> <a target="_blank" rel="nofollow noopener noreferrer"
                      href="https://www.youtube.com/@transcendingvictor">Youtube</a> </span>
        </div>
    <!-- Academic Box -->
    <div class="nav-box nav-box-academic">
        <ul>
            <li><a href="index.html">About</a></li>
            <li><a href="projects.html">Projects</a></li>
            <li><a href="publications.html">Publications</a></li>
            <li><a href="talks.html">Talks & Public</a></li>
            <li><a href="teaching.html">Teaching</a></li>
        </ul>
    </div>
    <!-- Personal Box -->
    <div class="nav-box nav-box-personal">
        <ul>
            <li><a href="self.html">Self</a></li>
            <li><a href="poems.html">Poems</a></li>
            <li><a href="reflective.html">Reflective Writing</a></li>
            <li><a href="youtube.html">YouTube</a></li>
        </ul>
    </div>
    </header>

    <div class="mainbody">
        <div class="sc mright"> <strong> last modified: mar, <span class="dsc"> 2025 </span> </strong> </div>
        <h1>Publications</h1>
        <div class="publication">
            <img src="resources\img\aggregation.png" alt="Publication Image">
            <div class="pub-content">
                <h3>Finding our common moral values: Guidelines for value system aggregation</h3>
                <div class="authors"> <b>Víctor Abia Alonso</b>, Marc Serramia Amoros, Eduardo Alonso </div>
                <div class="abstract">
                    The research community has already produced a breadth of ap-
                    proaches to resolve several value alignment problems. However,
                    in the pursuit of value alignment, we usually need to know which
                    values we want our AI to align with. This problem, called value
                    inference, has caught some attention lately with many approaches
                    to detect which moral values are relevant in a context, or to build a
                    model (called value system) representing the values and priorities of
                    an individual. However, another important task in value inference
                    is that of value system aggregation. This consists in aggregating
                    the moral value models of several individuals to obtain one rep-
                    resenting everybody. So far, only one value system aggregation
                    method has been proposed. In this paper, we discuss why research
                    in value system aggregation is paramount and the possible avenues
                    to implement value system aggregation depending on the value
                    alignment problem at hand.
                </div>
                <div class="conference-published"> <i> Workshop Paper under Review, 2024</i></div>
                <a class="pdf-link" href="./resources/publications/sample.pdf" target="_blank"> no PDF yet</a>
            </div>
        </div>

        <div class="publication">
            <img src="resources\img\spurious_paper_image.png" alt="Publication Image">
            <div class="pub-content">
                <h3>Reinforcement Learning Fine-tuning of Language
                    Models is Biased Towards More Extractable Features</h3>
                <div class="authors">Diogo Cruz, Edoardo Pona, Alex Holness-Tofts, Elias Schmied, <b>Víctor Abia Alonso</b>, Charlie Griffin, Bogdan-Ionut Cirstea.
                    </div>
                <div class="abstract">
                    Many capable large language models (LLMs) are developed via self-supervised
                    pre-training followed by a reinforcement-learning fine-tuning phase, often based
                    on human or AI feedback. During this stage, models may be guided by their
                    inductive biases to rely on simpler features which may be easier to extract, at a
                    cost to robustness and generalisation. We investigate whether principles governing
                    inductive biases in the supervised fine-tuning of LLMs also apply when the finetuning process uses reinforcement learning. Following Lovering et al. [2021], we
                    test two hypotheses: that features more extractable after pre-training are more
                    likely to be utilised by the final policy, and that the evidence for/against a feature
                    predicts whether it will be utilised. Through controlled experiments on synthetic
                    and natural language tasks, we find statistically significant correlations which
                    constitute strong evidence for these hypotheses.
                </div>
                <div class="conference-published"> <i> Socially Responsible Language Modelling Research (SoLaR) 2023 </i></div>
                
                <a class="pdf-link" href="resources\pdf\AI Safety Hub Labs paper.pdf" target="_blank">PDF</a>
            </div>
        </div>
        <!-- <div class="publication">
            <img src="resources\img\victor_mesa_phd.jpg" alt="Publication Image">
            <div class="pub-content">
                <h3>Title of Your Publication</h3>
                <div class="authors">Author 1, Author 2, Author 3</div>
                <div class="abstract">
                    This is a brief abstract of your publication. It provides a concise summary of your research, main findings, and significance of the study.
                </div>
                <a class="pdf-link" href="./resources/publications/sample.pdf" target="_blank"> PDF</a>
            </div>
        </div>
         -->

    </div>
</body>
</html>